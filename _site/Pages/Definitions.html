<!doctype html>
<html lang=" en-US">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title> Fairness Measures - Detecting Algorithmic Discrimination</title>

    <link rel="stylesheet"
          href="/assets/css/style.css?v=638069990a084005bd9f64127b5b8efbb7ab2e4c">
    <meta name="viewport" content="width=device-width">

    <meta name="keywords"
          content="algorithmic bias, algorithmic fairness, fairness benchmarking, dataset, machine learning, discrimination discovery"/>
    <meta name="description" content="A Fairness Benchmarking Tool for Machine Learning"/>
    <meta name="robots" content="index/follow">

</head>
<body>
    <div class="wrapper">
        <header>
            <h1><a href="http://localhost:4000">Fairness Measures</a></h1>
            <p>Datasets and Software for Detecting Algorithmic Discrimination</p>

            <menu style="list-style: none; padding: 0;
            list-style-type: none;">
                <p><a href="http://localhost:4000">Home</a></p>
                <p><a href="http://localhost:4000/Pages/Definitions">Definitions</a></p>
                <p><a href="http://localhost:4000/Pages/Ranking">Ranking Algorithms</a></p>
                <p><a href="http://localhost:4000/Pages/Classification">Classification Algorithms</a></p>
                <p><a href="http://localhost:4000/Pages/Datasets">Datasets</a></p>
                <p><a href="http://localhost:4000/Pages/Videos">Videos</a></p>
                <p><a href="https://github.com/FairnessMeasures/fairness-measures-code" target="_blank">Code (on
                GitHub)</a> &#128279;</p>
                <p><a href="http://localhost:4000/Pages/About">About</a></p>
            </menu>

        </header>
        <section>
            <img src="http://localhost:4000/Resources/Hero.svg" alt="Header">
            </br></br></br>

            <h1 id="fairness-definitions-in-machine-learning">Fairness Definitions in Machine Learning</h1>
<p>What does it actually mean for an algorithm to be fair? Different researchers have
used different notions of algorithmic fairness. We provide here three different ways 
of classifying fairness.</p>

<h2 id="group-versus-individual-fairness">Group versus Individual Fairness</h2>
<h3 id="group-fairness">Group Fairness</h3>
<p>It is also refered to as statistical parity. It is a requirement that the protected
groups should be treated similarly to the advantaged group or the populations as a whole.</p>

<h3 id="individual-fairness">Individual Fairness</h3>
<p>It is a requirement that individuals should be treated consistently.</p>

<h4 id="comparison-between-group--individual-fairness">Comparison between Group &amp; Individual Fairness</h4>
<p>Group fairness does not consider the individual merits and may result in choosing the 
less qualified members of a group, whereas individual fairness assumes a similarity metric
of the individuals for the classification task at hand that is generally hard to find.</p>

<h2 id="user-versus-content-biases">User versus Content Biases</h2>
<h3 id="user-bias">User Bias</h3>
<p>This appears when different users receive different content based on user attributes
that should be protected, such as gender, race, ethnicity, or religion.</p>
<h3 id="content-bias">Content Bias</h3>
<p>It refers to biases in the information received by any user. Take for example,
when some aspect is disproportionately represented in a query result or in news feeds.</p>

<h2 id="direct-versus-indirect-discrimination">Direct versus Indirect Discrimination</h2>
<h3 id="direct-discrimination">Direct Discrimination</h3>
<p>This consists of rules or procedures that explicitly mention minority or disadvantaged
groups based on sensitive discriminatory attributes related to group membership.</p>
<h3 id="indirect-discrimination">Indirect Discrimination</h3>
<p>This consists of rules or procedures that, while not explicitly mentioning discriminatory
attributes, intentionally or unintentionally could generate discriminatory decisions. 
It exists due to the correlation of the non-discriminatory items with the discriminatory ones.</p>

<h2 id="fairness-algorithms">Fairness Algorithms</h2>
<h3 id="fair-ranking-algorithms">Fair Ranking Algorithms</h3>
<p>This type of algorithms is typically used to find the most suitable way of odering items.
It is useful when a query is large, since most people will not scan through the entire list.</p>

<h3 id="fair-classification-algorithms">Fair Classification Algorithms</h3>
<p>This type of algorithms tackles the problem of classification subject to 
fairness constraints with respect to pre-defined senstive attributes such as race or gender.</p>

<h3 id="references">References</h3>

<p>These definitions of fairness can be found in following literature:</p>

<p>Zehlike, Meike, et al. “<a href="https://dl.acm.org/citation.cfm?id=3132938">Fa* ir: A fair top-k ranking algorithm</a>” Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. doi:10.1145/3132847.3132938. 
<a href="Files/bib/Fair.bib">bibtex</a></p>

<p>Pitoura, Evaggelia et al. “<a href="https://arxiv.org/abs/1704.05730">On measuring Bias in Online Information</a>.”	doi:10.1145/3186549.3186553 . <a href="Files/bib/pitoura.bib">bibtex</a></p>

<p>Hajian, Sara et al. “<a href="https://dl.acm.org/citation.cfm?id=2945386">Algorithmic Bias: From Discrimination Discovery to Fairness-aware Data Mining</a>.” doi:10.1145/2939672.2945386 .<a href="Files/bib/hajian.bib">bibtex</a></p>



        </section>
        <footer>
            <p>
                <a href="http://www.upf.edu/"><img hspace="2" src="http://localhost:4000/Resources/upf_logo.png"
                        style="height: 30px; margin: 4px;" alt="UPF logo"></a>
                <a href="http://www.cit.tu-berlin.de"><img hspace="2" src="http://localhost:4000/Resources/tu_logo.png"
                        style="height: 30px; margin: 4px;" alt="TU Berlin logo"></a>
            </p>
            <!--
            <p>This project is maintained by <a href="https://github.com/MilkaLichtblau"> Meike Zehlike </a> and <a href="http://github.com/FairnessMeasures">Mohamed Megahed</a></p>
            -->
            <p>
                <small>All content available under a <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0 license</a> unless specified otherwise. 
                    <script>document.write(new Date().getFullYear())</script>
                </small>
            </p>
        </footer>
    </div>

    <script src="/assets/js/scale.fix.js"></script>



<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-101054904-1', 'auto');
    ga('send', 'pageview');
</script>


</body>
</html> 